{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering tasks\n",
    "This file containes the descriptions of the solutions for all the 3 tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession initialization\n",
    "Defining the entry point into all functionality in Spark by creating a `SparkSession` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@3dafd0f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://dusan:4042)\" target=\"new_tab\">Spark UI: local-1575207444697</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1575207444697: Some(http://dusan:4042)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "            .appName(\"Data Engineering Test\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sales and rentals broadcast rights\n",
    "In task 1 the two datasets, i.e. `whatson` and `started_streams` are joined based on two joint conditions. The first condition implies that the variable `house_number` should be the identical in both datasets. The other condition requires the countries to be the same. Then, the requested columns are selected from the joined dataset and the same dataset is filtered making the date from the first dataset(`started_streams`) being in the broadcast_right range of whatson data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 DataFrame and extraction of requested data\n",
    "Two `DataFrame`'s are created based on the content of `whatson.csv` and `started_streams.csv` files, and named `whatson_df` and `streams_df` respectively.\n",
    "To be able to join them based on the `house_number` and `country_code`(`broadcast_right_region` for `whatson_df`) the values in the data has to be comparable (e.g. se should map to Sweden).\n",
    "In order to achieve that, `countryCodeMap` maps a country to its corresponding code to make it possible to compare the countries of the two datasets.\n",
    "The dataframes are filtered based on the `broadcast_right_vod_type` for `whatson_df` and `product_type` for `streams_df` so that only the data with `svod` or `tvod` is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99829\n",
      "875224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countryCodeMap = keys: [Norway,Sweden,Denmark,Finland], values: [no,se,dk,fi]\n",
       "whatson_df = [dt: string, house_number: string ... 6 more fields]\n",
       "streams_df = [dt: string, time: string ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[dt: string, time: string ... 9 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.lower\n",
    "import org.apache.spark.sql.functions.typedLit\n",
    "import org.apache.spark.sql.Column\n",
    "\n",
    "val countryCodeMap: Column = typedLit(Map(\n",
    "  \"Norway\" -> \"no\",\n",
    "  \"Sweden\"-> \"se\",\n",
    "  \"Denmark\" -> \"dk\",\n",
    "  \"Finland\" -> \"fi\"\n",
    "))\n",
    "\n",
    "val whatson_df = spark.read.format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(\"data/whatson.csv\")\n",
    "                .withColumn(\"broadcast_right_vod_type\",lower($\"broadcast_right_vod_type\"))\n",
    "                .filter($\"broadcast_right_vod_type\" === \"tvod\" || $\"broadcast_right_vod_type\" === \"svod\")\n",
    "                .withColumn(\"broadcast_right_region\", countryCodeMap($\"broadcast_right_region\"))\n",
    "\n",
    "val streams_df = spark.read.format(\"csv\")\n",
    "                .option(\"delimiter\",\";\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(\"data/started_streams.csv\")\n",
    "                .filter($\"product_type\" === \"tvod\" || $\"product_type\" === \"svod\")\n",
    "                \n",
    "//whatson_df.select(\"broadcast_right_region\").distinct().show()\n",
    "//streams_df.select(\"country_code\").distinct().show()\n",
    "\n",
    "//println(whatson_df.select(\"title\").distinct().count())\n",
    "//println(streams_df.select(\"program_title\").distinct().count())\n",
    "\n",
    "//whatson_df.select(\"product_category\").distinct().show()\n",
    "//streams_df.select(\"genre\").distinct().show()\n",
    "\n",
    "//streams_df.select(\"dt\").distinct().show()\n",
    "//whatson_df.select(\"dt\").distinct().show()\n",
    "\n",
    "println(streams_df.count()) // |99 829|\n",
    "println(whatson_df.count()) // |875 224|\n",
    "\n",
    "//streams_df.printSchema()\n",
    "//whatson_df.printSchema()\n",
    "\n",
    "//whatson_df.select(\"broadcast_right_vod_type\").distinct().show()\n",
    "//streams_df.select(\"product_type\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Join \n",
    "The inner join is performed based on a `house_number` and country.\n",
    "After that, the majority of respective columns from `streams_df` are selected and only keeping `broadcase_right` columns from `whatson`, given the desired outcome of the task.  \n",
    "For the simplicity reasons, `whatson_df` and `streams_df` got aliases `df1` and `df2` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1571021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joined_df = [dt: string, time: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[dt: string, time: string ... 11 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joined_df = whatson_df.as(\"df1\").join(streams_df.as(\"df2\"),\n",
    "                               $\"df1.house_number\"===$\"df2.house_number\"&&\n",
    "                               $\"df1.broadcast_right_region\"===$\"df2.country_code\", \"inner\")\n",
    "                            .select(\"df2.dt\",\n",
    "                                    \"df2.time\",\n",
    "                                    \"df2.device_name\",\n",
    "                                    \"df2.house_number\",\n",
    "                                    \"df2.user_id\",\n",
    "                                    \"df2.country_code\",\n",
    "                                    \"df2.program_title\",\n",
    "                                    \"df2.season\",\n",
    "                                    \"df2.season_episode\",\n",
    "                                    \"df2.genre\",\n",
    "                                    \"df2.product_type\",\n",
    "                                    \"df1.broadcast_right_start_date\",\n",
    "                                    \"df1.broadcast_right_end_date\")\n",
    "                            \n",
    "\n",
    "println(joined_df.count()) // |1 571 021|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Filtering out old dates \n",
    "The final dataframe is obtained by filtering out the `dt`(dates from `started_streams`) that are outiside of range `broadcast_right_start_date` and `broadcast_right_end_date`. \n",
    "Lastly, the results are saved localy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931268\n",
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- device_name: string (nullable = true)\n",
      " |-- house_number: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- program_title: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- season_episode: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- product_type: string (nullable = true)\n",
      " |-- broadcast_right_start_date: string (nullable = true)\n",
      " |-- broadcast_right_end_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "final_broadcast_df = [dt: string, time: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[dt: string, time: string ... 11 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val final_broadcast_df = joined_df.filter($\"dt\".between($\"broadcast_right_start_date\",\n",
    "                                                        $\"broadcast_right_end_date\"))\n",
    "\n",
    "//final_broadcast_df.agg(min(\"dt\")).show() |2018-10-01|\n",
    "//final_broadcast_df.agg(max(\"dt\")).show() |2018-10-01|\n",
    "\n",
    "//final_broadcast_df.agg(min(\"broadcast_right_start_date\"),        |1900-01-01|\n",
    "//                       max(\"broadcast_right_start_date\")).show() |2018-10-01|\n",
    "\n",
    "//final_broadcast_df.agg(min(\"broadcast_right_end_date\"),          |2018-10-01|\n",
    "//                       max(\"broadcast_right_end_date\")).show()   |2100-12-31|\n",
    "\n",
    "final_broadcast_df.write\n",
    "                  .csv(\"task1/output\")\n",
    "//println(final_broadcast_df.count()) // |931 268|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Product and user count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "streams_df = [dt: string, time: string ... 9 more fields]\n",
       "watches_count = [program_title: string, content_count: bigint]\n",
       "unique_users = [program_title: string, program_title: string ... 1 more field]\n",
       "streams_counts_df = [program_title: string, dt: string ... 10 more fields]\n",
       "product_user_df = [dt: string, program_title: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[dt: string, program_title: string ... 5 more fields]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val streams_df = spark.read.format(\"csv\")\n",
    "                .option(\"delimiter\",\";\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(\"data/started_streams.csv\")\n",
    "\n",
    "val watches_count = streams_df.groupBy(\"program_title\")\n",
    "                              .count()\n",
    "                              .withColumnRenamed(\"count\", \"content_count\")\n",
    "\n",
    "val unique_users = streams_df.groupBy(\"program_title\")\n",
    "                             .agg('program_title, countDistinct('user_id).as(\"unique_users\"))\n",
    "\n",
    "val streams_counts_df =  streams_df.join(watches_count, Seq(\"program_title\"))\n",
    "val product_user_df =  streams_counts_df.join(unique_users,Seq(\"program_title\"))\n",
    "                                        .select(\"dt\",\n",
    "                                                \"program_title\",\n",
    "                                                \"device_name\",\n",
    "                                                \"country_code\",\n",
    "                                                \"product_type\",\n",
    "                                                \"unique_users\",\n",
    "                                                \"content_count\")\n",
    "\n",
    "product_user_df.write\n",
    "                  .csv(\"task2/output\")\n",
    "\n",
    "\n",
    "// println(streams_df.count()) |100 000|\n",
    "// println(product_user_df.count()) |100 000|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Genre and time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
