{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering tasks\n",
    "This file containes the solutions with corresponding description for all the 3 tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession initialization\n",
    "Defining the entry point into all functionality in Spark by creating a `SparkSession` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@26c472c9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://192.168.0.17:4043)\" target=\"new_tab\">Spark UI: local-1575294584928</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1575294584928: Some(http://192.168.0.17:4043)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "            .appName(\"Data Engineering Test\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sales and rentals broadcast rights\n",
    "In task 1 the two datasets, i.e. `whatson` and `started_streams` are joined based on two joint conditions. The first condition implies that the variable `house_number` should be the identical in both datasets. The other condition requires the countries to be the same. Then, the requested columns are selected from the joined dataset and the same dataset is filtered making the date from the first dataset(`started_streams`) being in the broadcast_right range of whatson data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 DataFrame and extraction of requested data\n",
    "Two `DataFrame`'s are created based on the content of `whatson.csv` and `started_streams.csv` files, and named `whatson_df` and `streams_df` respectively.\n",
    "To be able to join them based on the `house_number` and `country_code`(`broadcast_right_region` for `whatson_df`) the values in the data has to be comparable (e.g. se should map to Sweden).\n",
    "In order to achieve that, `countryCodeMap` maps a country to its corresponding code to make it possible to compare the countries of the two datasets.\n",
    "The dataframes are filtered based on the `broadcast_right_vod_type` for `whatson_df` and `product_type` for `streams_df` so that only the data with `svod` or `tvod` is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99829\n",
      "875224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countryCodeMap = keys: [Norway,Sweden,Denmark,Finland], values: [no,se,dk,fi]\n",
       "whatson_df = [dt: string, house_number: string ... 6 more fields]\n",
       "streams_df = [dt: string, time: string ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[dt: string, time: string ... 9 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.lower\n",
    "import org.apache.spark.sql.functions.typedLit\n",
    "import org.apache.spark.sql.Column\n",
    "\n",
    "val countryCodeMap: Column = typedLit(Map(\n",
    "  \"Norway\" -> \"no\",\n",
    "  \"Sweden\"-> \"se\",\n",
    "  \"Denmark\" -> \"dk\",\n",
    "  \"Finland\" -> \"fi\"\n",
    "))\n",
    "\n",
    "// Input\n",
    "val whatson_df = spark.read.format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(\"data/whatson.csv\")\n",
    "                .withColumn(\"broadcast_right_vod_type\",lower($\"broadcast_right_vod_type\"))\n",
    "                .filter($\"broadcast_right_vod_type\" === \"tvod\" || $\"broadcast_right_vod_type\" === \"svod\")\n",
    "                .withColumn(\"broadcast_right_region\", countryCodeMap($\"broadcast_right_region\"))\n",
    "//Input\n",
    "val streams_df = spark.read.format(\"csv\")\n",
    "                .option(\"delimiter\",\";\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(\"data/started_streams.csv\")\n",
    "                .filter($\"product_type\" === \"tvod\" || $\"product_type\" === \"svod\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Join \n",
    "The inner join is performed based on a `house_number` and country.\n",
    "After that, the majority of respective columns from `streams_df` are selected and only keeping `broadcase_right` columns from `whatson`, given the desired outcome of the task.  \n",
    "For the simplicity reasons, `whatson_df` and `streams_df` got aliases `df1` and `df2` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1571021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joined_df = [dt: string, time: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[dt: string, time: string ... 11 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joined_df = whatson_df.as(\"df1\").join(streams_df.as(\"df2\"),\n",
    "                               $\"df1.house_number\"===$\"df2.house_number\"&&\n",
    "                               $\"df1.broadcast_right_region\"===$\"df2.country_code\", \"inner\")\n",
    "                            .select(\"df2.dt\",\n",
    "                                    \"df2.time\",\n",
    "                                    \"df2.device_name\",\n",
    "                                    \"df2.house_number\",\n",
    "                                    \"df2.user_id\",\n",
    "                                    \"df2.country_code\",\n",
    "                                    \"df2.program_title\",\n",
    "                                    \"df2.season\",\n",
    "                                    \"df2.season_episode\",\n",
    "                                    \"df2.genre\",\n",
    "                                    \"df2.product_type\",\n",
    "                                    \"df1.broadcast_right_start_date\",\n",
    "                                    \"df1.broadcast_right_end_date\")\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Filtering out old dates \n",
    "The final dataframe is obtained by filtering out the `dt`(dates from `started_streams`) that are outiside of range `broadcast_right_start_date` and `broadcast_right_end_date`. \n",
    "Lastly, the results are saved localy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_broadcast_df = [dt: string, time: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[dt: string, time: string ... 11 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val final_broadcast_df = joined_df.filter($\"dt\".between($\"broadcast_right_start_date\",\n",
    "                                                        $\"broadcast_right_end_date\"))\n",
    "                                  .distinct() // Removes duplicates, 931 268 -> 109 642\n",
    "\n",
    "final_broadcast_df.write\n",
    "                  .csv(\"task1/output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Product and user count\n",
    "In task 2, each product is getting the number of total views presented in column `content_count` and the number of unique users in column `unique_users`.\n",
    "In both cases, function `groupBy` is used to group the users depending on the same values from a column `program_title`. For the `content_count` a simple call to `count()` function is enough to calculate the amount of views each `program_title` has. On the other side, calculating the unique users per each `program_title` reqired a call to `agg()` function in order to be able to sum up only the unique users watching respetive program.\n",
    "Those two results are later joined in the `streams_df` dataframe to get the final table with all columns included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "streams_df = [dt: string, time: string ... 9 more fields]\n",
       "watches_count = [program_title: string, content_count: bigint]\n",
       "unique_users = [program_title: string, program_title: string ... 1 more field]\n",
       "streams_counts_df = [program_title: string, dt: string ... 10 more fields]\n",
       "product_user_df = [dt: string, program_title: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[dt: string, program_title: string ... 5 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "//Input\n",
    "val streams_df = spark.read.format(\"csv\")\n",
    "                .option(\"delimiter\",\";\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(\"data/started_streams.csv\")\n",
    "\n",
    "val watches_count = streams_df.groupBy(\"program_title\")\n",
    "                              .count()\n",
    "                              .withColumnRenamed(\"count\", \"content_count\")\n",
    "\n",
    "val unique_users = streams_df.groupBy(\"program_title\")\n",
    "                             .agg('program_title, countDistinct('user_id).as(\"unique_users\"))\n",
    "\n",
    "val streams_counts_df =  streams_df.join(watches_count, Seq(\"program_title\"))\n",
    "val product_user_df =  streams_counts_df.join(unique_users,Seq(\"program_title\"))\n",
    "                                        .select(\"dt\",\n",
    "                                                \"program_title\",\n",
    "                                                \"device_name\",\n",
    "                                                \"country_code\",\n",
    "                                                \"product_type\",\n",
    "                                                \"unique_users\",\n",
    "                                                \"content_count\")\n",
    "                                        .distinct() // removes duplicates, 100 000 -> 18 454\n",
    "\n",
    "\n",
    "product_user_df.write\n",
    "               .csv(\"task2/output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Genre and time of day\n",
    "In the final task, the goal is to calculate the number of unique users per genre and watched hour and finally rank according to the count of unique users.\n",
    "First, the data grouped on `genre` and `watched_hour` and than `agg()` function is used to calculate the number of unique users per each pair of `genre` and `watched_hour`. The outcome is later joned with `streams_df` (dataframe for `started_streams` data) and desired columns are selected for final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "streams_df = [dt: string, watched_hour: int ... 9 more fields]\n",
       "unique_users_hour_genre = [genre: string, watched_hour: int ... 2 more fields]\n",
       "users_per_genre_hour = [watched_hour: int, genre: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[watched_hour: int, genre: string ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Input\n",
    "val streams_df = spark.read.format(\"csv\")\n",
    "                .option(\"delimiter\",\";\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(\"data/started_streams.csv\")\n",
    "                .withColumn(\"time\", hour($\"time\"))\n",
    "                .withColumnRenamed(\"time\", \"watched_hour\")\n",
    "    \n",
    "// Grouping the data based on numbers of distinct users per watched hour and genre\n",
    "val unique_users_hour_genre = streams_df.groupBy(\"genre\",\"watched_hour\")\n",
    "                             .agg('genre, countDistinct('user_id).as(\"unique_users\"))\n",
    "\n",
    "val users_per_genre_hour =  streams_df.join(unique_users_hour_genre, Seq(\"genre\",\"watched_hour\"))\n",
    "          .orderBy(desc(\"unique_users\"))\n",
    "          .select(\"watched_hour\",\n",
    "                  \"genre\",\n",
    "                  \"unique_users\")\n",
    "          .distinct() //Removes duplicates, 100 000 -> 192\n",
    "         \n",
    "\n",
    "users_per_genre_hour.write\n",
    "              .csv(\"task3/output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
